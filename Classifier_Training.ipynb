{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9626961,"sourceType":"datasetVersion","datasetId":5876497}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport random\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision.models import convnext_base\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom tqdm import tqdm\nimport copy\nimport gc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T03:52:45.005337Z","iopub.execute_input":"2025-07-05T03:52:45.006089Z","iopub.status.idle":"2025-07-05T03:52:45.011833Z","shell.execute_reply.started":"2025-07-05T03:52:45.006054Z","shell.execute_reply":"2025-07-05T03:52:45.010961Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class LoadDataset(Dataset):\n    def __init__(self, low_dir, high_dir, max=None, image_size = (360,360), augment=False):\n        self.image_paths = []\n        self.labels = []\n        self.augment = augment\n        self.max = max\n\n        low_images = [os.path.join(low_dir,f) for f in os.listdir(low_dir)if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n        low_images = random.sample(low_images, max)\n        \n        high_images = [os.path.join(high_dir,f) for f in os.listdir(high_dir)if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n        high_images = random.sample(high_images, max)\n\n        self.image_paths += low_images\n        self.labels += [1] * len(low_images)\n\n        self.image_paths += high_images\n        self.labels += [0] * len(high_images)\n\n        combined = list(zip(self.image_paths, self.labels))\n        random.shuffle(combined)\n        self.image_paths, self.labels = zip(*combined)\n\n        self.base_transforms = transforms.Compose([transforms.Resize(image_size), transforms.ToTensor()])\n        self.augmentation = transforms.Compose([transforms.RandomHorizontalFlip(), \n                                              transforms.ColorJitter(brightness = 0.4, contrast = 0.4, saturation = 0.4, hue = 0.2),\n                                              transforms.GaussianBlur(kernel_size = 3, sigma = (0.1,2.0)),\n                                              transforms.RandomAffine(degrees = 5, translate = (0.25,0.25))])\n    def __len__(self):\n        return len(self.image_paths)\n        \n    def __getitem__(self, idx):\n        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n\n        if self.augment:\n            img = self.augmentation(img)\n\n        img = self.base_transforms(img)\n        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n        return img, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T03:52:47.361176Z","iopub.execute_input":"2025-07-05T03:52:47.361807Z","iopub.status.idle":"2025-07-05T03:52:47.370048Z","shell.execute_reply.started":"2025-07-05T03:52:47.361781Z","shell.execute_reply":"2025-07-05T03:52:47.369261Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"base_path = \"/kaggle/input/loli-street-low-light-image-enhancement-of-street\"\nbase_path = base_path + \"/LoLI-Street Dataset\"\n\ntrain_low_dir = os.path.join(base_path, 'Train', 'low')\ntrain_high_dir = os.path.join(base_path, 'Train', 'high')\n\ntest_low_dir = os.path.join(base_path, 'Val', 'low')\ntest_high_dir = os.path.join(base_path, 'Val', 'high')\n\ntrain_dataset = LoadDataset(low_dir=train_low_dir,high_dir=train_high_dir,max = 10000,image_size=(360,360),augment=True)\ntest_dataset = LoadDataset(low_dir=test_low_dir,high_dir=test_high_dir,max = 200,image_size=(360,360),augment=False)\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-05T03:52:50.180839Z","iopub.execute_input":"2025-07-05T03:52:50.181498Z","iopub.status.idle":"2025-07-05T03:52:50.927672Z","shell.execute_reply.started":"2025-07-05T03:52:50.181474Z","shell.execute_reply":"2025-07-05T03:52:50.927127Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from torchvision.models import ConvNeXt_Base_Weights\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = convnext_base(weights=ConvNeXt_Base_Weights.DEFAULT)\nmodel.classifier[2] = nn.Linear(model.classifier[2].in_features, 1)\nmodel = model.to(device)\n\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\nscheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n\nnum_epochs = 7\nbest_val_acc = 0.0\nbest_val_loss = float('inf')\nbest_model_wts = copy.deepcopy(model.state_dict())\n\nfor epoch in range(num_epochs):\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    model.train()\n    train_loss, train_correct, total = 0.0, 0, 0\n\n    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n        inputs, labels = inputs.to(device), labels.to(device).unsqueeze(1).float()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item() * inputs.size(0)\n        preds = (torch.sigmoid(outputs) > 0.5).float()\n        train_correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\n    train_acc = train_correct / total\n    avg_train_loss = train_loss / total\n\n    model.eval()\n    val_loss, val_correct, total = 0.0, 0, 0\n\n    with torch.no_grad():\n        for inputs, labels in tqdm(test_loader, desc=f\"Epoch {epoch+1} Validation\"):\n            inputs, labels = inputs.to(device), labels.to(device).unsqueeze(1).float()\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            val_loss += loss.item() * inputs.size(0)\n            preds = (torch.sigmoid(outputs) > 0.5).float()\n            val_correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n    val_acc = val_correct / total\n    avg_val_loss = val_loss / total\n\n    print(f\"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n\n    scheduler.step(val_acc)\n    torch.save(model.state_dict(), f\"convnext_epoch_{epoch+1}.pt\")\n\n    if (val_acc > best_val_acc) or (val_acc == best_val_acc and avg_val_loss < best_val_loss):\n        best_val_acc = val_acc\n        best_val_loss = avg_val_loss\n        best_model_wts = copy.deepcopy(model.state_dict())\n        torch.save(best_model_wts, \"Classifier.pt\")\n\nmodel.load_state_dict(best_model_wts)\nprint(f\"Training complete. Best Val Acc: {best_val_acc:.4f}, Best Val Loss: {best_val_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T03:52:54.001265Z","iopub.execute_input":"2025-07-05T03:52:54.001571Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/convnext_base-6075fbad.pth\" to /root/.cache/torch/hub/checkpoints/convnext_base-6075fbad.pth\n100%|██████████| 338M/338M [00:01<00:00, 211MB/s] \nEpoch 1 Training:  20%|█▉        | 977/5000 [13:57<56:36,  1.18it/s]  ","output_type":"stream"}],"execution_count":null}]}